{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ca0b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "#\n",
    "# reference configuration\n",
    "#\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "stop = EarlyStopping(monitor='val_sp', mode='max', verbose=1, patience=25, restore_best_weights=True)\n",
    "\n",
    "import datetime, os\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "logdir = os.path.join('.', 'logs/%s' %(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "\n",
    "loss              = 'binary_crossentropy'\n",
    "metrics           = 'accuracy'\n",
    "batch_size        = 500\n",
    "callbacks         = [stop, tensorboard]\n",
    "epochs            = 50\n",
    "class_weight      = True\n",
    "sorts             = 1\n",
    "inits             = 1\n",
    "optimizer       = 'adam'\n",
    "epochs          = 50\n",
    "job_auto_config      = 'jobConfig/job_config.ID_0000.ml0.mu5_sl0.su0_il0.iu0.29-Aug-2021-17.28.16.pic.gz'\n",
    "sorts           = range(1)\n",
    "inits           = 1\n",
    "__verbose       = True\n",
    "__model_generator= None\n",
    "total = 100000\n",
    "background_percent = 0.99\n",
    "test_size = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c028da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: ROOT not found. You will not be able to use the TEventLoop, storage and monet  services provied by the gaugi core.\n"
     ]
    }
   ],
   "source": [
    "from Gaugi.messenger import Logger\n",
    "from Gaugi.messenger.macros import *\n",
    "from Gaugi import StatusCode, checkForUnusedVars, retrieve_kw\n",
    "from six import print_\n",
    "\n",
    "from tensorflow.keras.models import clone_model\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "import pandas as pd\n",
    "\n",
    "def lock_as_completed_job(output):\n",
    "    with open(output+'/.complete','w') as f:\n",
    "        f.write('complete')\n",
    "\n",
    "\n",
    "def lock_as_failed_job(output):\n",
    "    with open(output+'/.failed','w') as f:\n",
    "        f.write('failed')\n",
    "\n",
    "\n",
    "def get_data( total, background_percent, test_size ):\n",
    "\n",
    "    #########################################################\n",
    "    # ------------------------------------------------------ #\n",
    "    # ----------------------- LOADING ---------------------- #\n",
    "    # ------------------------------------------------------ #\n",
    "    ##########################################################\n",
    "    # Firstly the model loads the background and signal data, \n",
    "    # then it removes the attributes first string line, which \n",
    "    # are the column names, in order to avoid NaN values in \n",
    "    # the array.\n",
    "\n",
    "    print('==== Commencing Initiation ====\\n')\n",
    "\n",
    "    ### Background\n",
    "    b_name='/home/thiago/Documents/Data_Sets/LPC-anomaly-detection/Input_Background_1.csv'\n",
    "    background = np.genfromtxt(b_name, delimiter=',')\n",
    "    background = background[1:,:]\n",
    "    print(\".Background Loaded...\" )\n",
    "    print(\".Background shape: {}\".format(background.shape))\n",
    "\n",
    "    ### Signal\n",
    "    s_name='/home/thiago/Documents/Data_Sets/LPC-anomaly-detection/Input_Signal_1.csv'\n",
    "    signal = np.genfromtxt(s_name, delimiter=',')\n",
    "    signal = signal[1:,:]\n",
    "    print(\".Signal Loaded...\")\n",
    "    print(\".Signal shape: {}\\n\".format(signal.shape))\n",
    "\n",
    "    ##########################################################\n",
    "    # ------------------------------------------------------ #\n",
    "    # --------------------- INITIATION --------------------- #\n",
    "    # ------------------------------------------------------ #\n",
    "    ##########################################################\n",
    "\n",
    "    print('\\n          ==== Initiation Complete ====\\n')\n",
    "    print('=*='*17 )\n",
    "    print('\\n      ==== Commencing Pre-processing ====\\n')\n",
    "\n",
    "    # Percentage of background samples to divide the data-set\n",
    "    dat_set_percent = total/len(background)\n",
    "\n",
    "    # Reducing background samples\n",
    "    _,reduced_background = train_test_split(background, test_size=dat_set_percent)\n",
    "\n",
    "    # Deviding train and test background\n",
    "\n",
    "    train_data, background_test = train_test_split(reduced_background, test_size=test_size)\n",
    "\n",
    "    # Iserting the correct number of signal in streaming\n",
    "\n",
    "    n_signal_samples = int(len(background_test)*(1-background_percent))\n",
    "\n",
    "    _,background_test = train_test_split(background_test, test_size=background_percent)\n",
    "\n",
    "    _,signal_test = train_test_split(signal, test_size=n_signal_samples/len(signal))\n",
    "\n",
    "    # Concatenating Signal and the Background sub-sets\n",
    "\n",
    "    test_data = np.vstack((background_test,signal_test))\n",
    "\n",
    "    # Normalize Data\n",
    "\n",
    "    print('.Normalizing Data')\n",
    "\n",
    "    test_data = normalize(test_data,\n",
    "                            norm='max',\n",
    "                            axis=0\n",
    "                        )\n",
    "\n",
    "    train_data = normalize(train_data,\n",
    "                            norm='max',\n",
    "                            axis=0\n",
    "                        )\n",
    "\n",
    "    # Creates test data frame\n",
    "\n",
    "    attributes = np.array([\"px1\",\n",
    "                            \"py1\",\n",
    "                            \"pz1\",\n",
    "                            \"E1\",\n",
    "                            \"eta1\",\n",
    "                            \"phi1\",\n",
    "                            \"pt1\",\n",
    "                            \"px2\",\n",
    "                            \"py2\",\n",
    "                            \"pz2\",\n",
    "                            \"E2\",\n",
    "                            \"eta2\",\n",
    "                            \"phi2\",\n",
    "                            \"pt2\",\n",
    "                            \"Delta_R\",\n",
    "                            \"M12\",\n",
    "                            \"MET\",\n",
    "                            \"S\",\n",
    "                            \"C\",\n",
    "                            \"HT\",\n",
    "                            \"A\"]\n",
    "                        )\n",
    "\n",
    "    test_df = pd.DataFrame(test_data,columns = attributes)\n",
    "\n",
    "    # Creating Labels\n",
    "    print('.Creating Labels')\n",
    "\n",
    "    test_labels =np.ones((len(test_data)))\n",
    "    test_labels[:len(background_test)] = 0\n",
    "\n",
    "    print('\\n      ==== Pre-processing Complete ====\\n')\n",
    "    print(\".Train data shape: {}\".format(train_data.shape))\n",
    "    print(\".Test data shape: {}\".format(test_data.shape))\n",
    "    print(\".Test Background shape: {}\".format(background_test.shape))\n",
    "    print(\".Test Signal shape: {}\".format(signal_test.shape))\n",
    "\n",
    "    print('=*='*17 )\n",
    "\n",
    "    return train_data,test_data,test_df,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330fe90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for isort, sort in enumerate( sorts ):\n",
    "\n",
    "    # get the current kfold and train, val sets\n",
    "    train_data,test_data,test_df,test_labels = get_data(total, \n",
    "                                                        background_percent, \n",
    "                                                        test_size)\n",
    "    for imodel, model in enumerate( __models ):\n",
    "\n",
    "        for iinit, init in enumerate(inits):\n",
    "\n",
    "            print(model)\n",
    "            # get the model \"ptr\" for this sort, init and model index\n",
    "            if __model_generator:\n",
    "                print(  \"Apply model generator...\" )\n",
    "                model_for_this_init = __model_generator( sort )\n",
    "            else: \n",
    "                model_for_this_init = clone_model(model) # get only the model\n",
    "\n",
    "\n",
    "            try:\n",
    "                model_for_this_init.compile( self.optimizer,\n",
    "                            loss = self.loss,\n",
    "                            # protection for functions or classes with internal variables\n",
    "                            # this copy avoid the current training effect the next one.\n",
    "                            metrics = deepcopy(self.metrics),\n",
    "                            #metrics = self.metrics,\n",
    "                            )\n",
    "                model_for_this_init.summary()\n",
    "            except RuntimeError as e:\n",
    "                print(\"Compilation model error: %s\" , e)\n",
    "\n",
    "\n",
    "            print(\"Training model id (%d) using sort (%d) and init (%d)\", self.__id_models[imodel], sort, init )\n",
    "\n",
    "            callbacks = deepcopy(self.callbacks)\n",
    "\n",
    "            start = datetime.now()\n",
    "            \n",
    "            # Hacn: used by orchestra to set this job as local test\n",
    "            if os.getenv('LOCAL_TEST'): \n",
    "                print(  \"The LOCAL_TEST environ was detected.\" )\n",
    "                print(  \"This is a short local test, lets skip the fitting for now. \")\n",
    "                return StatusCode.SUCCESS\n",
    "\n",
    "            Train = [train_data, np.ones(len(train_data))]\n",
    "\n",
    "            # Training\n",
    "            history = model_for_this_init.fit(x = Train, \n",
    "                                y = train_data,\n",
    "                                epochs          = self.epochs,\n",
    "                                batch_size      = self.batch_size,\n",
    "                                verbose         = self.__verbose,\n",
    "                                validation_split= 0.1,\n",
    "                                callbacks       = callbacks,\n",
    "                                shuffle         = True).history\n",
    "\n",
    "            end = datetime.now()\n",
    "\n",
    "            self.__context.setHandler(\"time\" , end-start)\n",
    "\n",
    "            self.history = history\n",
    "\n",
    "            # Clear everything for the next init\n",
    "            K.clear_session()\n",
    "\n",
    "    # Clear the keras once again just to be sure\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59155a63",
   "metadata": {},
   "source": [
    "# Saves the models in .json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e726ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stamp = time_stamp_func()    \n",
    "# creating the job mechanism file first\n",
    "mkdir_p(outputFolder)\n",
    "\n",
    "if type(models) is not list:\n",
    "    models = [models]\n",
    "\n",
    "modelJobsWindowList = create_iter(lambda i, sorts: list(range(i, i+sorts)), \n",
    "                                  nModelsPerJob,\n",
    "                                  len(models))\n",
    "sortJobsWindowList  = create_iter(lambda i, sorts: list(range(i, i+sorts)), \n",
    "                                  nSortsPerJob,\n",
    "                                  sortBounds)\n",
    "initJobsWindowList  = create_iter(lambda i, sorts: list(range(i, i+sorts)), \n",
    "                                  nInitsPerJob, \n",
    "                                  nInits)\n",
    "\n",
    "nJobs = 0 \n",
    "for (model_idx_list, sort_list, init_list) in product(modelJobsWindowList,\n",
    "                                                      sortJobsWindowList, \n",
    "                                                      initJobsWindowList):\n",
    "    job = Job_v1()\n",
    "    # to be user by the database table\n",
    "    job.setId( nJobs )\n",
    "    job.setSorts(sort_list)\n",
    "    job.setInits(init_list)\n",
    "    job.setModels([models[idx] for idx in model_idx_list],  model_idx_list )\n",
    "    # save config file\n",
    "    model_str = 'ml%i.mu%i' %(model_idx_list[0], model_idx_list[-1])\n",
    "    sort_str  = 'sl%i.su%i' %(sort_list[0], sort_list[-1])\n",
    "    init_str  = 'il%i.iu%i' %(init_list[0], init_list[-1])\n",
    "    job.save( outputFolder+'/' + ('job_config.ID_%s.%s_%s_%s.%s') %\n",
    "          ( str(nJobs).zfill(4), model_str, sort_str, init_str, time_stamp) )\n",
    "    nJobs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6b0219",
   "metadata": {},
   "source": [
    "# Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19b3e65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiago/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py:1761: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from tensorflow.compat.v1 import ConfigProto\n",
    "    from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "    config = ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = InteractiveSession(config=config)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Not possible to set gpu allow growth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f7a7a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPileup( path ):\n",
    "    return load(path)['data'][:,0]\n",
    "\n",
    "\n",
    "def getJobConfigId( path ):\n",
    "    return dict(load(path))['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f4a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    job_id = getJobConfigId( args.configFile )\n",
    "    \n",
    "    mkdir_p('results')\n",
    "\n",
    "    outputFile = './results/tunedDiscr.jobID_%s'%str(job_id).zfill(4)\n",
    "\n",
    "    from saphyra.decorators import Summary, Reference\n",
    "    decorators = [Summary(), Reference(args.refFile, targets)]\n",
    "\n",
    "    from saphyra.callbacks import sp\n",
    "\n",
    "\n",
    "    from saphyra import PatternGenerator\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from saphyra.applications import BinaryClassificationJob\n",
    "\n",
    "\n",
    "    job = BinaryClassificationJob(  PatternGenerator( args.dataFile, getPatterns ),\n",
    "                                  StratifiedKFold(n_splits=10, random_state=512, shuffle=True),\n",
    "                                  job               = args.configFile,\n",
    "                                  loss              = 'mean_squared_error',\n",
    "                                  metrics           = ['accuracy'],\n",
    "                                  callbacks         = [sp(patience=25, verbose=True, save_the_best=True)],\n",
    "                                  epochs            = 5000,\n",
    "                                  class_weight      = False,\n",
    "                                  outputFile        = outputFile )\n",
    "    ############# AQUI É ONDE EU DEVO ADICIONAR O OPTIMIZER (optimizer = 'adam')\n",
    "    # modificar o ADAM com o SetMembership e ele sera um parametro da função BinaryClassificationJob\n",
    "\n",
    "    job.decorators += decorators\n",
    "\n",
    "\n",
    "    # Run it!\n",
    "    job.run()\n",
    "\n",
    "\n",
    "    # necessary to work on orchestra\n",
    "    from saphyra import lock_as_completed_job\n",
    "    lock_as_completed_job(args.volume if args.volume else '.')\n",
    "\n",
    "    sys.exit(0)\n",
    "\n",
    "except  Exception as e:\n",
    "    print(e)\n",
    "\n",
    "    # necessary to work on orchestra\n",
    "    from saphyra import lock_as_failed_job\n",
    "    lock_as_failed_job(args.volume if args.volume else '.')\n",
    "\n",
    "    sys.exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
