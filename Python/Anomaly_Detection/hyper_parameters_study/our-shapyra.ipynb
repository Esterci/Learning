{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25ca0b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from Gaugi import retrieve_kw, mkdir_p\n",
    "from Gaugi.messenger import Logger\n",
    "from Gaugi.messenger.macros import *\n",
    "from Gaugi import load\n",
    "from itertools import product\n",
    "from keras.layers import Dense, Input, Concatenate, Flatten, BatchNormalization, Dropout, LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb20b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# ------------------------------------------------------ #\n",
    "# --------------------- INITIATION --------------------- #\n",
    "# ------------------------------------------------------ #\n",
    "##########################################################\n",
    "\n",
    "# Number of events\n",
    "total = 100000\n",
    "\n",
    "# Percentage of background samples on the testing phase\n",
    "background_percent = 0.99\n",
    "\n",
    "# Percentage of samples on the training phase\n",
    "test_size = 0.3\n",
    "\n",
    "# Number of iterations\n",
    "\n",
    "n_it = 33\n",
    "\n",
    "# Defining hyper-parameters range\n",
    "\n",
    "min_batch_size = 200\n",
    "\n",
    "max_batch_size = 500\n",
    "\n",
    "min_hidden_dim = 8\n",
    "\n",
    "max_hidden_dim = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d136be44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters in study\n",
    "\n",
    "batch_size_list = list(np.linspace(min_batch_size,max_batch_size,num=3,dtype=int))\n",
    "encoding_dim_list = list(np.linspace(max_hidden_dim,min_hidden_dim,num=3,dtype=int))\n",
    "lambda_disco_list = list(np.linspace(0,600,num=3,dtype=int))\n",
    "act_func_list_1 = ['relu',\n",
    "                   'sigmoid',\n",
    "                   #'softmax',\n",
    "                   #'softplus',\n",
    "                   #'softsign',\n",
    "                   'tanh',\n",
    "                   #'selu',\n",
    "                   #'elu',\n",
    "                   #'exponential'\n",
    "                   ]\n",
    "act_func_list_2 = act_func_list_1\n",
    "act_func_list_3 = act_func_list_1\n",
    "\n",
    "n_combinations = (len(batch_size_list) * \n",
    "                    len(encoding_dim_list) * \n",
    "                    len(lambda_disco_list) * \n",
    "                    len(act_func_list_1) * \n",
    "                    len(act_func_list_1) * \n",
    "                    len(act_func_list_1)\n",
    "                )\n",
    "\n",
    "modelCol = []\n",
    "\n",
    "for batch_size in batch_size_list:\n",
    "    for encoding_dim in encoding_dim_list:\n",
    "        for lambda_disco in lambda_disco_list:\n",
    "            for act_1 in act_func_list_1:\n",
    "                for act_2 in act_func_list_2:\n",
    "                    for act_3 in act_func_list_3:\n",
    "\n",
    "                        # Fixed parameters\n",
    "\n",
    "                        nb_epoch = 100\n",
    "                        input_dim = 21\n",
    "                        hidden_dim_1 = int(encoding_dim / 2)\n",
    "                        hidden_dim_2 = int(hidden_dim_1 / 2)\n",
    "                        learning_rate = 0.001\n",
    "\n",
    "                        ###### Creates NN structure #####\n",
    "                        \n",
    "                        #input Layer\n",
    "                        input_layer = Input(shape=(input_dim, ))\n",
    "                        sample_weights = Input(shape=(1, ))\n",
    "                        #Encoder\n",
    "                        encoder = tf.keras.layers.Dense(encoding_dim, \n",
    "                                                        activation=act_1,\n",
    "                                activity_regularizer=tf.keras.regularizers.l2(learning_rate)\n",
    "                                                    )(input_layer)\n",
    "                        \n",
    "                        encoder = tf.keras.layers.Dropout(0.2)(encoder)\n",
    "                        \n",
    "                        encoder = tf.keras.layers.Dense(hidden_dim_1, \n",
    "                                                        activation=act_2\n",
    "                                                    )(encoder)\n",
    "                        \n",
    "                        encoder = tf.keras.layers.Dense(hidden_dim_2, \n",
    "                                                        activation=act_3\n",
    "                                                    )(encoder)\n",
    "                        # Decoder\n",
    "                        decoder = tf.keras.layers.Dense(hidden_dim_1, \n",
    "                                                        activation=act_3\n",
    "                                                    )(encoder)\n",
    "                        \n",
    "                        decoder=tf.keras.layers.Dropout(0.2)(decoder)\n",
    "                        \n",
    "                        decoder = tf.keras.layers.Dense(encoding_dim, \n",
    "                                                        activation=act_2\n",
    "                                                    )(decoder)\n",
    "                        \n",
    "                        decoder = tf.keras.layers.Dense(input_dim, \n",
    "                                                        activation=act_1\n",
    "                                                    )(decoder)\n",
    "                        #Autoencoder\n",
    "                        autoencoder = tf.keras.Model(inputs=[input_layer, sample_weights], \n",
    "                                                    outputs=decoder\n",
    "                                                    )\n",
    "                        \n",
    "                        modelCol.append(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2a2a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    modelCol = []\n",
    "    for n in range(10, 15):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(n, input_shape=(100,), activation='tanh', name='dense_layer'))\n",
    "        model.add(Dense(1, activation='linear', name='output_for_inference'))\n",
    "        model.add(Activation('tanh', name='output_for_training'))\n",
    "        modelCol.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76713928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_stamp_func():\n",
    "    from datetime import datetime\n",
    "    dateTimeObj = datetime.now()\n",
    "    timestampStr = dateTimeObj.strftime(\"%d-%b-%Y-%H.%M.%S\")\n",
    "    return timestampStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92c8ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_iter(fun, n_items_per_job, items_lim):\n",
    "    return ([fun(i, n_items_per_job)\n",
    "           if (i+n_items_per_job) <= items_lim \n",
    "           else fun(i, items_lim % n_items_per_job) \n",
    "           for i in range(0, items_lim, n_items_per_job)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "383e328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['Job_v1']\n",
    "\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from Gaugi import LoggerStreamable, LoggerRawDictStreamer, RawDictCnv\n",
    "# Just to remove the keras dependence\n",
    "import tensorflow as tf\n",
    "model_from_json = tf.keras.models.model_from_json\n",
    "\n",
    "import json\n",
    "\n",
    "class Job_v1( LoggerStreamable ):\n",
    "\n",
    "    _streamerObj = LoggerRawDictStreamer(toPublicAttrs = {'_metadata','_id' , '_sorts', '_inits', '_models'})\n",
    "    _cnvObj = RawDictCnv(toProtectedAttrs = {'_metadata','_id', '_sorts', '_inits', '_models'})\n",
    "\n",
    "    __version =  1\n",
    "\n",
    "    def __init__( self, **kw ):\n",
    "        LoggerStreamable.__init__(self, kw)\n",
    "        self._sorts  = []\n",
    "        self._inits  = []\n",
    "        self._models = []\n",
    "        self._id     = None\n",
    "        self._metadata = None\n",
    "\n",
    "    def setSorts(self, v):\n",
    "        if type(v) is int:\n",
    "            self._sorts = [v]\n",
    "        else:\n",
    "            self._sorts = v\n",
    "\n",
    "\n",
    "    def setInits(self, v):\n",
    "        if type(v) is int:\n",
    "            self._inits = range(v)\n",
    "        else:\n",
    "            self._inits = v\n",
    "\n",
    "\n",
    "    def getSorts(self):\n",
    "        return self._sorts\n",
    "\n",
    "\n",
    "    def getInits(self):\n",
    "        return self._inits\n",
    "\n",
    "\n",
    "    def setMetadata( self, d):\n",
    "        self._metadata = d\n",
    "\n",
    "\n",
    "    def getMetadata(self):\n",
    "        return self._metadata\n",
    "\n",
    "\n",
    "    def setModels(self, models, id_models):\n",
    "        self._models = list()\n",
    "        if type(models) is not list:\n",
    "            models=[models]\n",
    "        for idx, model in enumerate(models):\n",
    "            self._models.append({'model':  json.loads(model.to_json()), \n",
    "                                'weights': model.get_weights() , \n",
    "                                'id_model': id_models[idx]})\n",
    "\n",
    "\n",
    "    def getModels(self):\n",
    "        # Loop over all keras model\n",
    "        models = []; id_models = []\n",
    "        for d in self._models:\n",
    "            model = model_from_json( json.dumps(d['model'], \n",
    "                                    separators=(',', ':')) , \n",
    "                                    custom_objects={'RpLayer':RpLayer})\n",
    "            model.set_weights( d['weights'] )\n",
    "            models.append( model )\n",
    "            id_models.append( d['id_model'] )\n",
    "        return models, id_models\n",
    "\n",
    "\n",
    "    def setId( self, id ):\n",
    "        self._id = id\n",
    "\n",
    "\n",
    "    def id(self):\n",
    "        return self._id\n",
    "\n",
    "\n",
    "    def save(self, fname):\n",
    "        d = self.toRawObj()\n",
    "        d['__version'] = self.__version\n",
    "        from Gaugi import save\n",
    "        save( d, fname, compress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb19748f",
   "metadata": {},
   "source": [
    "# Creates the model architeture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39af4974",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = modelCol\n",
    "nInits = 1\n",
    "nInitsPerJob = 1\n",
    "sortBounds = 1\n",
    "nSortsPerJob = 1\n",
    "nModelsPerJob = 5\n",
    "outputFolder = 'job-config'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59155a63",
   "metadata": {},
   "source": [
    "# Saves the models in .json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e726ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stamp = time_stamp_func()    \n",
    "# creating the job mechanism file first\n",
    "mkdir_p(outputFolder)\n",
    "\n",
    "if type(models) is not list:\n",
    "    models = [models]\n",
    "\n",
    "modelJobsWindowList = create_iter(lambda i, sorts: list(range(i, i+sorts)), \n",
    "                                  nModelsPerJob,\n",
    "                                  len(models))\n",
    "sortJobsWindowList  = create_iter(lambda i, sorts: list(range(i, i+sorts)), \n",
    "                                  nSortsPerJob,\n",
    "                                  sortBounds)\n",
    "initJobsWindowList  = create_iter(lambda i, sorts: list(range(i, i+sorts)), \n",
    "                                  nInitsPerJob, \n",
    "                                  nInits)\n",
    "\n",
    "nJobs = 0 \n",
    "for (model_idx_list, sort_list, init_list) in product(modelJobsWindowList,\n",
    "                                                      sortJobsWindowList, \n",
    "                                                      initJobsWindowList):\n",
    "    job = Job_v1()\n",
    "    # to be user by the database table\n",
    "    job.setId( nJobs )\n",
    "    job.setSorts(sort_list)\n",
    "    job.setInits(init_list)\n",
    "    job.setModels([models[idx] for idx in model_idx_list],  model_idx_list )\n",
    "    # save config file\n",
    "    model_str = 'ml%i.mu%i' %(model_idx_list[0], model_idx_list[-1])\n",
    "    sort_str  = 'sl%i.su%i' %(sort_list[0], sort_list[-1])\n",
    "    init_str  = 'il%i.iu%i' %(init_list[0], init_list[-1])\n",
    "    job.save( outputFolder+'/' + ('job_config.ID_%s.%s_%s_%s.%s') %\n",
    "          ( str(nJobs).zfill(4), model_str, sort_str, init_str, time_stamp) )\n",
    "    nJobs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6b0219",
   "metadata": {},
   "source": [
    "# Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19b3e65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiago/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py:1761: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from tensorflow.compat.v1 import ConfigProto\n",
    "    from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "    config = ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = InteractiveSession(config=config)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Not possible to set gpu allow growth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f7a7a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPileup( path ):\n",
    "    return load(path)['data'][:,0]\n",
    "\n",
    "\n",
    "def getJobConfigId( path ):\n",
    "    return dict(load(path))['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f4a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    job_id = getJobConfigId( args.configFile )\n",
    "    \n",
    "    mkdir_p('results')\n",
    "\n",
    "    outputFile = './results/tunedDiscr.jobID_%s'%str(job_id).zfill(4)\n",
    "\n",
    "    from saphyra.decorators import Summary, Reference\n",
    "    decorators = [Summary(), Reference(args.refFile, targets)]\n",
    "\n",
    "    from saphyra.callbacks import sp\n",
    "\n",
    "\n",
    "    from saphyra import PatternGenerator\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from saphyra.applications import BinaryClassificationJob\n",
    "\n",
    "\n",
    "    job = BinaryClassificationJob(  PatternGenerator( args.dataFile, getPatterns ),\n",
    "                                  StratifiedKFold(n_splits=10, random_state=512, shuffle=True),\n",
    "                                  job               = args.configFile,\n",
    "                                  loss              = 'mean_squared_error',\n",
    "                                  metrics           = ['accuracy'],\n",
    "                                  callbacks         = [sp(patience=25, verbose=True, save_the_best=True)],\n",
    "                                  epochs            = 5000,\n",
    "                                  class_weight      = False,\n",
    "                                  outputFile        = outputFile )\n",
    "    ############# AQUI É ONDE EU DEVO ADICIONAR O OPTIMIZER (optimizer = 'adam')\n",
    "    # modificar o ADAM com o SetMembership e ele sera um parametro da função BinaryClassificationJob\n",
    "\n",
    "    job.decorators += decorators\n",
    "\n",
    "\n",
    "    # Run it!\n",
    "    job.run()\n",
    "\n",
    "\n",
    "    # necessary to work on orchestra\n",
    "    from saphyra import lock_as_completed_job\n",
    "    lock_as_completed_job(args.volume if args.volume else '.')\n",
    "\n",
    "    sys.exit(0)\n",
    "\n",
    "except  Exception as e:\n",
    "    print(e)\n",
    "\n",
    "    # necessary to work on orchestra\n",
    "    from saphyra import lock_as_failed_job\n",
    "    lock_as_failed_job(args.volume if args.volume else '.')\n",
    "\n",
    "    sys.exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
