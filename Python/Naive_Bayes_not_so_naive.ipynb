{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "narrow-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-supervision",
   "metadata": {},
   "source": [
    "# Bayes Theorem of Conditional Probability\n",
    "Before we dive into Bayes theorem, let’s review marginal, joint, and conditional probability.\n",
    "\n",
    "Recall that marginal probability is the probability of an event, irrespective of other random variables. If the random variable is independent, then it is the probability of the event directly, otherwise, if the variable is dependent upon other variables, then the marginal probability is the probability of the event summed over all outcomes for the dependent variables, called the sum rule.\n",
    "\n",
    " - __Marginal Probability__: The probability of an event irrespective of the outcomes of other random variables, e.g. P(A).\n",
    " \n",
    "The joint probability is the probability of two (or more) simultaneous events, often described in terms of events A and B from two dependent random variables, e.g. X and Y. The joint probability is often summarized as just the outcomes, e.g. A and B.\n",
    "\n",
    " - __Joint Probability__: Probability of two (or more) simultaneous events, e.g. P(A and B) or P(A, B).\n",
    "\n",
    "The conditional probability is the probability of one event given the occurrence of another event, often described in terms of events A and B from two dependent random variables e.g. X and Y.\n",
    "\n",
    "- __Conditional Probability__: Probability of one (or more) event given the occurrence of another event, e.g. P(A given B) or P(A | B).\n",
    "\n",
    "The joint probability can be calculated using the conditional probability; for example:\n",
    "\n",
    "$P(A, B) = P(A | B) * P(B)$\n",
    "\n",
    "This is called the product rule. Importantly, the joint probability is symmetrical, meaning that:\n",
    "\n",
    "$P(A, B) = P(B, A)$\n",
    "\n",
    "The conditional probability can be calculated using the joint probability; for example:\n",
    "\n",
    "$P(A | B) = P(A, B) / P(B)$\n",
    "\n",
    "The conditional probability is not symmetrical; for example:\n",
    "\n",
    "$P(A | B) != P(B | A)$\n",
    "\n",
    "We are now up to speed with marginal, joint and conditional probability. If you would like more background on these fundamentals, see the tutorial:\n",
    "\n",
    " - __[A Gentle Introduction to Joint, Marginal, and Conditional Probability](https://machinelearningmastery.com/joint-marginal-and-conditional-probability-for-machine-learning/)__\n",
    "\n",
    "# An Alternate Way To Calculate Conditional Probability\n",
    "\n",
    "Now, there is another way to calculate the conditional probability.\n",
    "\n",
    "Specifically, one conditional probability can be calculated using the other conditional probability; for example:\n",
    "\n",
    "$P(A|B) = P(B|A) * P(A) / P(B)$\n",
    "\n",
    "The reverse is also true; for example:\n",
    "\n",
    "$P(B|A) = P(A|B) * P(B) / P(A)$\n",
    "\n",
    "This alternate approach of calculating the conditional probability is useful either when the joint probability is challenging to calculate (which is most of the time), or when the reverse conditional probability is available or easy to calculate.\n",
    "\n",
    "This alternate calculation of the conditional probability is referred to as Bayes Rule or Bayes Theorem, named for Reverend Thomas Bayes, who is credited with first describing it. It is grammatically correct to refer to it as Bayes’ Theorem (with the apostrophe), but it is common to omit the apostrophe for simplicity.\n",
    "\n",
    " - __Bayes Theorem__: Principled way of calculating a conditional probability without the joint probability.\n",
    "It is often the case that we do not have access to the denominator directly, e.g. P(B).\n",
    "\n",
    "We can calculate it an alternative way; for example:\n",
    "\n",
    "$P(B) = P(B|A) * P(A) + P(B|not A) * P(not A)$\n",
    "\n",
    "This gives a formulation of Bayes Theorem that we can use that uses the alternate calculation of P(B), described below:\n",
    "\n",
    "$P(A|B) = P(B|A) * P(A) / P(B|A) * P(A) + P(B|not A) * P(not A)$\n",
    "\n",
    "Or with brackets around the denominator for clarity:\n",
    "\n",
    "$P(A|B) = P(B|A) * P(A) / (P(B|A) * P(A) + P(B|not A) * P(not A))$\n",
    "\n",
    "Note: the denominator is simply the expansion we gave above.\n",
    "\n",
    "As such, if we have P(A), then we can calculate P(not A) as its complement; for example:\n",
    "\n",
    "$P(not A) = 1 – P(A)$\n",
    "\n",
    "Additionally, if we have P(not B|not A), then we can calculate P(B|not A) as its complement; for example:\n",
    "\n",
    "$P(B|not A) = 1 – P(not B|not A)$\n",
    "\n",
    "Now that we are familiar with the calculation of Bayes Theorem, let’s take a closer look at the meaning of the terms in the equation.\n",
    "\n",
    "# Naming the Terms in the Theorem\n",
    "\n",
    "The terms in the Bayes Theorem equation are given names depending on the context where the equation is used.\n",
    "\n",
    "It can be helpful to think about the calculation from these different perspectives and help to map your problem onto the equation.\n",
    "\n",
    "Firstly, in general, the result P(A|B) is referred to as the posterior probability and P(A) is referred to as the prior probability.\n",
    "\n",
    "$P(A|B): Posterior probability.$\n",
    "$P(A): Prior probability.$\n",
    "\n",
    "Sometimes P(B|A) is referred to as the likelihood and P(B) is referred to as the evidence.\n",
    "\n",
    "$P(B|A): Likelihood.$\n",
    "\n",
    "$P(B): Evidence.$\n",
    "\n",
    "This allows Bayes Theorem to be restated as:\n",
    "\n",
    "Posterior = Likelihood * Prior / Evidence\n",
    "We can make this clear with a smoke and fire case.\n",
    "\n",
    "> __What is the probability that there is fire given that there is smoke?__\n",
    "\n",
    "Where P(Fire) is the Prior, P(Smoke|Fire) is the Likelihood, and P(Smoke) is the evidence:\n",
    "\n",
    "$P(Fire|Smoke) = P(Smoke|Fire) * P(Fire) / P(Smoke)$\n",
    "\n",
    "You can imagine the same situation with rain and clouds.\n",
    "\n",
    "Now that we are familiar with Bayes Theorem and the meaning of the terms, let’s look at a scenario where we can calculate it.\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "\n",
    "The solution to using Bayes Theorem for a conditional probability classification model is to simplify the calculation.\n",
    "\n",
    "The Bayes Theorem assumes that each input variable is dependent upon all other variables. This is a cause of complexity in the calculation. We can remove this assumption and consider each input variable as being independent from each other.\n",
    "\n",
    "This changes the model from a dependent conditional probability model to an independent conditional probability model and dramatically simplifies the calculation.\n",
    "\n",
    "This means that we calculate P(data|class) for each input variable separately and multiple the results together, for example:\n",
    "\n",
    "$P(class | X1, X2, …, Xn) = P(X1|class) * P(X2|class) * … * P(Xn|class) * P(class) / P(data)$\n",
    "\n",
    "We can also drop the probability of observing the data as it is a constant for all calculations, for example:\n",
    "\n",
    "$P(class | X1, X2, …, Xn) = P(X1|class) * P(X2|class) * … * P(Xn|class) * P(class)$\n",
    "\n",
    "This simplification of Bayes Theorem is common and widely used for classification predictive modeling problems and is generally referred to as Naive Bayes.\n",
    "\n",
    "The word “naive” is French and typically has a diaeresis (umlaut) over the “i”, which is commonly left out for simplicity, and “Bayes” is capitalized as it is named for Reverend Thomas Bayes.\n",
    "\n",
    "For tutorials on how to implement Naive Bayes from scratch in Python see:\n",
    "\n",
    " - __[How to Develop a Naive Bayes Classifier from Scratch in Python](https://machinelearningmastery.com/classification-as-conditional-probability-and-the-naive-bayes-algorithm/)__\n",
    " \n",
    "## Original Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "informed-diagram",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 -> P(y=0 | [-0.79415228  2.10495117]) = 0.348\n",
      "X1 -> Truth: y=1\n",
      "\n",
      "X2 -> P(y=0 | [-9.15155186 -4.81286449]) = 0.000\n",
      "X2 -> Truth: y=0\n"
     ]
    }
   ],
   "source": [
    "# example of preparing and making a prediction with a naive bayes model\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.stats import norm\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "# fit a probability distribution to a univariate data sample\n",
    "def fit_distribution(data):\n",
    "    # estimate parameters\n",
    "    mu = mean(data)\n",
    "    sigma = std(data)\n",
    "    # fit distribution\n",
    "    dist = norm(mu, sigma)\n",
    "    return dist\n",
    "\n",
    "# calculate the independent conditional probability\n",
    "def probability(X, prior, dist1, dist2):\n",
    "    return prior * dist1.pdf(X[0]) * dist2.pdf(X[1])\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
    "# sort data into classes\n",
    "Xy0 = X[y == 0]\n",
    "y0 = y[y==0]\n",
    "Xy1 = X[y == 1]\n",
    "y1 = y[y==1]\n",
    "# calculate priors\n",
    "priory0 = len(Xy0) / len(X)\n",
    "priory1 = len(Xy1) / len(X)\n",
    "# create PDFs for y==0\n",
    "distX1y0 = fit_distribution(Xy0[:, 0])\n",
    "distX2y0 = fit_distribution(Xy0[:, 1])\n",
    "# create PDFs for y==1\n",
    "distX1y1 = fit_distribution(Xy1[:, 0])\n",
    "distX2y1 = fit_distribution(Xy1[:, 1])\n",
    "# classify one example\n",
    "X1sample, y1sample = Xy0[0], y0[0]\n",
    "X2sample, y2sample = Xy1[0], y1[0]\n",
    "\n",
    "X1py0 = probability(X1sample, priory0, distX1y0, distX2y0)\n",
    "X2py0 = probability(X2sample, priory0, distX1y0, distX2y0)\n",
    "\n",
    "print('X1 -> P(y=0 | %s) = %.3f' % (X1sample, X1py0*100))\n",
    "print('X1 -> Truth: y=%d' % np.sqrt((y1sample-1)**2))\n",
    "print('\\nX2 -> P(y=0 | %s) = %.3f' % (X2sample, X2py0*100))\n",
    "print('X2 -> Truth: y=%d' % np.sqrt((y2sample-1)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-circulation",
   "metadata": {},
   "source": [
    "## Non-supervised version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rough-comedy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Normal Distribution\n",
      "X1 -> P(y=0 | [-0.79415228  2.10495117]) = 0.348\n",
      "X1 -> Truth: y=1\n",
      "\n",
      "X2 -> P(y=0 | [-9.15155186 -4.81286449]) = 0.000\n",
      "X2 -> Truth: y=0\n",
      "\n",
      "Using KDE\n",
      "X1 -> P(y=0 | [-0.79415228  2.10495117]) = 0.622\n",
      "X1 -> Truth: y=1\n",
      "\n",
      "X2 -> P(y=0 | [-9.15155186 -4.81286449]) = 0.000\n",
      "X2 -> Truth: y=0\n"
     ]
    }
   ],
   "source": [
    "# example of preparing and making a prediction with a naive bayes model\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.stats import norm,gaussian_kde\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "# fit a probability distribution to a univariate data sample\n",
    "def fit_distribution(data):\n",
    "    # estimate parameters\n",
    "    mu = mean(data)\n",
    "    sigma = std(data)\n",
    "    # fit distribution\n",
    "    dist = norm(mu, sigma)\n",
    "    return dist\n",
    "\n",
    "# calculate the independent conditional probability\n",
    "def probability(X, prior, dist1, dist2):\n",
    "    return prior * dist1.pdf(X[0]) * dist2.pdf(X[1])\n",
    "\n",
    "# calculate the independent conditional probability\n",
    "def probability_kde (X, prior, dist1, dist2):\n",
    "    return prior * dist1(X[0]) * dist2(X[1])\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
    "# sort data into classes\n",
    "Xy0 = X[y == 0]\n",
    "y0 = y[y==0]\n",
    "Xy1 = X[y == 1]\n",
    "y1 = y[y==1]\n",
    "# calculate priors\n",
    "priory0 = len(Xy0) / len(X)\n",
    "# create PDFs for y==0\n",
    "seed_kde1 = gaussian_kde(Xy0[:, 0])\n",
    "seed_kde2 = gaussian_kde(Xy0[:, 1])\n",
    "distX1y0 = fit_distribution(Xy0[:, 0])\n",
    "distX2y0 = fit_distribution(Xy0[:, 1])\n",
    "# classify one example\n",
    "X1sample, y1sample = Xy0[0], y0[0]\n",
    "X2sample, y2sample = Xy1[0], y1[0]\n",
    "\n",
    "X1py0 = probability(X1sample, priory0, distX1y0, distX2y0)\n",
    "X2py0 = probability(X2sample, priory0, distX1y0, distX2y0)\n",
    "\n",
    "print('\\nUsing Normal Distribution')\n",
    "print('X1 -> P(y=0 | %s) = %.3f' % (X1sample, X1py0*100))\n",
    "print('X1 -> Truth: y=%d' % np.sqrt((y1sample-1)**2))\n",
    "print('\\nX2 -> P(y=0 | %s) = %.3f' % (X2sample, X2py0*100))\n",
    "print('X2 -> Truth: y=%d' % np.sqrt((y2sample-1)**2))\n",
    "\n",
    "X1py0 = probability_kde(X1sample, priory0, seed_kde1, seed_kde2)\n",
    "X2py0 = probability_kde(X2sample, priory0, seed_kde1, seed_kde2)\n",
    "\n",
    "print('\\nUsing KDE')\n",
    "print('X1 -> P(y=0 | %s) = %.3f' % (X1sample, X1py0*100))\n",
    "print('X1 -> Truth: y=%d' % np.sqrt((y1sample-1)**2))\n",
    "print('\\nX2 -> P(y=0 | %s) = %.3f' % (X2sample, X2py0*100))\n",
    "print('X2 -> Truth: y=%d' % np.sqrt((y2sample-1)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-delivery",
   "metadata": {},
   "source": [
    "# Final function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "determined-times",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n",
      "0.050993807837646836\n",
      "(50,)\n",
      "4.23308571425288e-74\n",
      "(100,)\n",
      "0.02549690391882341\n"
     ]
    }
   ],
   "source": [
    "def kde_naive_bayes (X, kde_dict, prior):\n",
    "    # calculate the independent conditional probability\n",
    "    L,_ = np.shape(X)\n",
    "    prob = np.ones((L)) * prior\n",
    "        \n",
    "    for i in range(L):\n",
    "        for kde in kde_dict:\n",
    "            prob[i] *= kde_dict[kde](X[i,kde])\n",
    "    return prob\n",
    "\n",
    "def kde_dictionary (X):\n",
    "    _,W = np.shape(X)\n",
    "    # Creating KDE dictionary\n",
    "    s = set(range(W))\n",
    "    kde_dict = dict.fromkeys(s)\n",
    "    for kde in kde_dict:\n",
    "        kde_dict[kde] = gaussian_kde(X[:, kde])\n",
    "    return kde_dict\n",
    "\n",
    "seed_kde_dict = kde_dictionary(Xy0)\n",
    "\n",
    "print(np.shape(kde_naive_bayes(Xy0,seed_kde_dict,priory0)))\n",
    "print(kde_naive_bayes(Xy0,seed_kde_dict,priory0).mean())\n",
    "print(np.shape(kde_naive_bayes(Xy1,seed_kde_dict,priory0)))\n",
    "print(kde_naive_bayes(Xy1,seed_kde_dict,priory0).mean())\n",
    "print(np.shape(kde_naive_bayes(X,seed_kde_dict,priory0)))\n",
    "print(kde_naive_bayes(X,seed_kde_dict,priory0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-status",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
